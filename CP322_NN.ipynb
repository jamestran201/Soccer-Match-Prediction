{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_1, weights['output']) + biases['output']\n",
    "    return out_layer\n",
    "\n",
    "def neural_net_2_layers(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['output']) + biases['output']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file):\n",
    "    filename = [file]\n",
    "    record_defaults = [tf.float32] * 75   # Eight required float columns\n",
    "    dataset = tf.contrib.data.CsvDataset(filename, record_defaults)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Data/formated.csv\")\n",
    "test_df = pd.read_csv(\"Data/formatedTesting.csv\")\n",
    "\n",
    "train_df.columns = [col.strip() for col in train_df.columns]\n",
    "test_df.columns = [col.strip() for col in test_df.columns]\n",
    "\n",
    "input_nodes = 72 # maybe 75 for a bias\n",
    "output_nodes = 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Delete duplicate columns\n",
    "del train_df[\"e1\"]\n",
    "del train_df[\"e1.1\"]\n",
    "del test_df[\"e1\"]\n",
    "del test_df[\"e1.1\"]\n",
    "\n",
    "# Uncomment this block to try out different set of features\n",
    "feature_subset = [\"assist count\", \"assist count 2\", \"shots made\", \"e6\", \"is goal 2\", \"s1.1\",\n",
    "                            \"e6.1\", \"is goal\", \"e4\", \"e2\", \"shots made 2\", \"s2.1\", \"e2.1\", \"target Feature\"]\n",
    "train_df = train_df.loc[:, feature_subset]\n",
    "test_df = test_df.loc[:, feature_subset]\n",
    "input_nodes = 13 # maybe 75 for a bias\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, input_nodes])\n",
    "Y = tf.placeholder(\"float\", [None, output_nodes])\n",
    "\n",
    "# Split the dataframe into features and labels arrays\n",
    "train_features_array = train_df.iloc[:, :-1].values\n",
    "train_labels_array = train_df.loc[:, \"target Feature\"].values\n",
    "train_labels_binarized = label_binarize(train_labels_array, classes=[0,1,2])\n",
    "print(train_labels_binarized)\n",
    "test_features_array = test_df.iloc[:, :-1].values\n",
    "test_labels_array = test_df.loc[:, \"target Feature\"].values\n",
    "test_labels_binarized = label_binarize(test_labels_array, classes=[0,1,2])\n",
    "\n",
    "normalizer = MinMaxScaler((0,1))\n",
    "normalizer.fit(train_features_array)\n",
    "\n",
    "train_features_normalized = normalizer.transform(train_features_array)\n",
    "test_features_normalized = normalizer.transform(test_features_array)\n",
    "\n",
    "train_features_normalized, vald_features_normalized, train_labels_binarized, vald_labels_binarized = train_test_split(train_features_normalized, train_labels_binarized, test_size=0.10, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hLayers = 2\n",
    "# hLayerOne = 36 # half of input nodes\n",
    "# hLayerTwo = 36 # for sigmoid function\n",
    "\n",
    "\n",
    "# #epochs = 1 # number of times to iterate over data\n",
    "# batches = 3\n",
    "# train_batch = int(len(train_df)/batches)\n",
    "# test_batch = int(len(test_df)/batches)\n",
    "# # keep_prob = tf.placeholder(\"float\")\n",
    "# # print(keep_prob)\n",
    "\n",
    "# print(X)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning network with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 2 units in layer 1\n",
      "Training accuracy:  0.75775385\n",
      "Validation accuracy:  0.7312775\n",
      "------------------------------\n",
      "Trying 4 units in layer 1\n",
      "Training accuracy:  0.75987595\n",
      "Validation accuracy:  0.7312775\n",
      "------------------------------\n",
      "Trying 6 units in layer 1\n",
      "Training accuracy:  0.76379365\n",
      "Validation accuracy:  0.72246695\n",
      "------------------------------\n",
      "Trying 8 units in layer 1\n",
      "Training accuracy:  0.75857\n",
      "Validation accuracy:  0.7283407\n",
      "------------------------------\n",
      "Trying 10 units in layer 1\n",
      "Training accuracy:  0.7605289\n",
      "Validation accuracy:  0.72540385\n",
      "------------------------------\n",
      "Optimal number of units in first hidden layer: 2\n",
      "Maximum validation accuracy: 0.7312775254249573\n"
     ]
    }
   ],
   "source": [
    "total_batch = train_features_array.shape[0]\n",
    "max_validation_accuracy = 0\n",
    "optimal_weights = {}\n",
    "optimal_n_units = 0\n",
    "\n",
    "n_layer_one_units = [i for i in range(2, 11, 2)]\n",
    "accuracies = {}\n",
    "\n",
    "for n_unit_1 in n_layer_one_units:\n",
    "    print(\"Trying {} units in layer 1\".format(n_unit_1))\n",
    "\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([input_nodes, n_unit_1]), name=\"h1\"),\n",
    "        'output': tf.Variable(tf.random_normal([n_unit_1, output_nodes]), name=\"output_weights\")\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_unit_1]), name=\"b1\"),\n",
    "        'output': tf.Variable(tf.random_normal([output_nodes]), name=\"output_bias\")\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    logits = neural_net(X)\n",
    "\n",
    "    # Finding the cost of the algorithm\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "    # Finding optimization of the algorithm\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(1, 1001):\n",
    "            avg_cost = 0.0\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                X: train_features_normalized, \n",
    "                                Y: train_labels_binarized, \n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "#             if i % 100 == 0:\n",
    "#                 print(\"Epoch:\", '%04d' % (i), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "#         print(\"Optimization Finished!\")            \n",
    "\n",
    "#         print()\n",
    "\n",
    "        train_pred = tf.nn.softmax(logits)\n",
    "        correct_train_prediction = tf.equal(tf.argmax(train_pred, 1), tf.argmax(Y, 1))\n",
    "        train_accuracy = tf.reduce_mean(tf.cast(correct_train_prediction, \"float\"))\n",
    "        print(\"Training accuracy: \", train_accuracy.eval({X: train_features_normalized, Y: train_labels_binarized}))\n",
    "\n",
    "        valid_pred = tf.nn.softmax(logits)\n",
    "        correct_valid_prediction = tf.equal(tf.argmax(valid_pred, 1), tf.argmax(Y, 1))\n",
    "        valid_accuracy = tf.reduce_mean(tf.cast(correct_valid_prediction, \"float\"))\n",
    "        valid_accuracy_value = valid_accuracy.eval({X: vald_features_normalized, Y: vald_labels_binarized})\n",
    "        accuracies[n_unit_1] = valid_accuracy_value\n",
    "        print(\"Validation accuracy: \", valid_accuracy_value)\n",
    "        \n",
    "        if max_validation_accuracy < valid_accuracy_value:\n",
    "            max_validation_accuracy = valid_accuracy_value\n",
    "            optimal_n_units = n_unit_1\n",
    "            optimal_weights[\"h1\"] = weights[\"h1\"].eval()\n",
    "            optimal_weights[\"output_weights\"] = weights[\"output\"].eval()\n",
    "            optimal_weights[\"b1\"] = biases[\"b1\"].eval()\n",
    "            optimal_weights[\"output_bias\"] = biases[\"output\"].eval()\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "print(\"Optimal number of units in first hidden layer: {}\".format(optimal_n_units))\n",
    "print(\"Maximum validation accuracy: {}\".format(max_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame({\"n_units\": n_layer_one_units, \"accuracy\": list(accuracies.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy on validation set')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFtCAYAAAA5/7CSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xd4XPWd7/H3V5J7ldz7GFywMZYMkpOQQBJaTLNJWUp2Q8jdJHdTgXCTkEa4KbvkbijZJA+bZDcFSOhgmRJKYIGQALbBMti4GxtLsrGMK+6WvvePc8Yei5E1kubMkWY+r+eZx1POnPMbSf7o6DfnfMbcHRERyb2iuAcgIlKoFMAiIjFRAIuIxEQBLCISEwWwiEhMFMAiIjFRAItkmZn93sx+FF4/zcxWZLJsO7f1jpkd197nS7wUwAXEzJ4xs21m1iPusRQKd/+ru0/OxrrC799nm62/r7uvzcb6O8rMEmbmZlYS91i6CgVwgTCzBHAa4MDsHG9b/yFF0lAAF47LgReB3wOfTn3AzHqZ2Y1mtt7MdpjZ82bWK3zsA2b2dzPbbmYbzOyK8P6j9sbM7Aozez7ltpvZl8xsFbAqvO9n4Tp2mtnLZnZayvLFZvZtM1tjZrvCx8eY2S/N7MZm433IzK5K9yLN7FQzWxC+jgVmdmrKY8+Y2Q/N7G/hNp4ws8EtrGeZmV2QcrvEzLaY2cnh7XvNbFO4nefM7MQW1vMhM6tNuT3DzF4Jt3830DPlsVIze9jMGsK/VB42s9HhYz8m+AX6i3Da4RcpX+cJ4fUBZnZb+Pz1ZvZdMytK/f6Y2U/Ddb9hZuemG3O4/DfNrC4c5wozOzO8v8jMrg2/T2+b2T1mVhY+7bnw3+3hGN/X0vol5O66FMAFWA18ETgFOAgMS3nsl8AzwCigGDgV6AGMBXYBlwHdgEFARficZ4DPpqzjCuD5lNsOPAmUAb3C+/4pXEcJcA2wCegZPvZ14DVgMmBAebjsTKAeKAqXGwzsSR1/yjbLgG3Ap8JtXBbeHpQy5jXAJKBXePuGFr5e1wF/TLl9PrA85fb/AvqFX6dbgJqUx34P/Ci8/iGgNrzeHVgPXB1+PT8Rfi+Syw4CPg70Dtd9LzA3Zb1Hfc1Tvs4Twuu3AdXhcxPASuCfU74/B4HPhd/jL4RfV0vz2icDG4CR4e0EcHx4/SqCX+Sjw9f+K+DOlOUcKIn7572rXGIfgC45+CbDB8L/fIPD28uBq8PrRcBeoDzN874FPNjCOo8KA9IH8BmtjGtbcrvACmBOC8stA84Or38ZeLSF5T4FzG923wvAFSlj/m7KY18EHmthXRMIfvn0Dm//EbiuhWUHhq93QHi7pQA+vXnoAX9PLptmvRXAtpa+5ilf5wlhqO4HpqY89r+BZ1K+P6tTHusdPnd4C699M3AW0C3N9+LMlNsjwp+tEgVw2y+agigMnwaecPct4e0/cWQaYjDBn8Fr0jxvTAv3Z2pD6g0zuyb8036HmW0HBoTbb21bfyDYeyb89/YWlhtJsIeZaj3Bnn3SppTre4C+6Vbk7qsJwuZCM+tNMG/+p/B1FJvZDeGf4TuBdeHT0k5nNBtfnYfJlTI+wvX2NrNfhdMHOwn+pB9oZsWtrDe57eQeduq60752d98TXn3X6w9f+1XA9cBmM7vLzEaGD48DHgynpLYTfI0agWEZjFGaUQDnuXAu92Lgg+Gc5SaCP4HLzawc2ALsA45P8/QNLdwPsJtgLyppeJplDgdNON/7zXAspe4+ENhBMN3Q2rbuAOaE450CzG1huXqCgEg1FqhrYfnW3EkwjTEHeD0MJoBPhvedRfBLJBHeb81X0MxGYJSZpS43NuX6NQR//r/H3fsT7DGnrvdY1YVbCPZEU19/u1+7u//J3T8Qrs+Bn4QPbQDOdfeBKZee7l7XyvgkDQVw/ruIYA9lKsGftBUEIfZX4HJ3bwJ+C9xkZiPDvbv3WXCo2h+Bs8zs4vBNqEFmVhGutwb4WLjXNgH451bG0Q84BDQAJWZ2HdA/5fH/An5oZhMtMN3MBgG4ey2wgGDP935339vCNh4FJpnZJ8PxXhK+7ocz/WI1cxdwDsF86Z+avZb9wNsEv4T+NcP1vUDwNfhqOL6PEcxxp653L8GbWGXA95s9/y0g7TG/7t4I3AP82Mz6mdk44GsEv7zaxMwmm9kZ4c/AvnBMjeHD/xluY1y47BAzmxM+1gA0tTRGeTcFcP77NPA7d3/T3TclL8AvgH+04BCx/0PwBtgCYCvB3k6Ru78JnEewZ7aVIHTLw/XeDBwgCIU/EIT1sTwO/JngjaH1BP+xU6cobiIIkCeAncB/E7xRlvQH4CRann7A3d8GLgjH+zbwDeCClKmXNnH3jQSheSpwd8pDt4WvoQ54neBNqUzWdwD4GMF87DbgEuCBlEVuIXjNW8J1PtZsFT8DPhEexfAfaTbxFYK/TNYCzxP80vhtJmNrpgdwQziOTcBQ4NspY5gHPGFmu8Jxvid8fXuAHwN/C6co3tuObRcUO3o6SqRzMrPTCfbmEuFeu0iXpz1g6fTMrBtwJfBfCl/JJwpg6dTMbAqwneBwp1tiHo5IVmkKQkQkJtoDFhGJiQJYRCQmBdFSNWvWLH/sseZH9IiIRKa1k3KAAtkD3rKlXYeBiohEqiACWESkM1IAi4jERAEsIhITBbCISEwUwCIiMVEAi4jERAEsIhITBbCISEwUwCIiMVEAi4jEpCC6INpq6+4DcQ8hdiXFRv+e3eIehkheUwCn8f4bnmbvwcbWF8xzv7m8krOn6tPGRaKiAE7jO+dPobGpsIvqf/r4Cp5ZsVkBLBIhBXAa//TecXEPIXZPL9/MwnXb4h6GSF7Tm3CS1szxZax4axc79hyMeygieUsBLGlVjisFYOH6rTGPRCR/KYAlrfIxA+lWbCzQNIRIZBTAklbPbsWcNGoAC9dpD1gkKgpgaVHV+DJerd3BPh2SJxIJBbC0qGpcGQcam3i1dkfcQxHJSwpgadEp4RtxCzQNIRIJBbC0qLRPdyYO7at5YJGIKIDlmKrGl7Fw/TaaCvzMQJEoKIDlmKoSpezad4gVb+2KeygieUcBLMdUOa4MQNMQIhFQAMsxjS7txfD+PXVChkgEFMByTGYWzANrD1gk6xTA0qqqRCn1O/ZRu21P3EMRySsKYGnVkXlgTUOIZJMCWFo1eXg/+vUo0QkZIlmmAJZWFRcZpyRKFcAiWaYAloxUJcpY+dY7bN+jDywVyRYFsGQkWdD+8nrNA4tkiwJYMqKCdpHsUwBLRnp2K2b66IGaBxbJIgWwZKwyUcqrtdtV0C6SJQpgyVjVuDIONroK2kWyRAEsGatMqKBdJJsUwJKxgb27M2lYXwWwSJYogKVNKhNlvLx+G40qaBfpMAWwtEmyoH2lCtpFOkwBLG1SlVBBu0i2KIClTUYN7MWIAT2ZrxMyRDpMASxtYmZUJspY8MZW3DUPLNIRCmBps6pEKZt27qNu+964hyLSpSmApc2OzANrGkKkIxTA0maThvWjX88S5uuNOJEOUQBLmxUXGaeMK9WRECIdFGkAm9ksM1thZqvN7No0j99sZjXhZaWZbQ/vH2dmL4f3LzWzf0l5zilm9lq4zv8wM4vyNUh6KmgX6bjIAtjMioFfAucCU4HLzGxq6jLufrW7V7h7BfBz4IHwoY3AqeH97wGuNbOR4WO3Ap8HJoaXWVG9BmlZch5YBe0i7RflHvBMYLW7r3X3A8BdwJxjLH8ZcCeAux9w9/3h/T2S4zSzEUB/d3/Bg2OgbgMuiuoFSMumjx5A9+IizQOLdECUATwK2JByuza8713MbBwwHng65b4xZvZquI6fuHt9+PzaTNYp0erZrZiTRg/QkRAiHRBlAKebm23pyP1Lgfvc/XDTt7tvcPfpwATg02Y2rC3rNLPPm9lCM1vY0NDQxqFLJlTQLtIxUQZwLTAm5fZooL6FZS8lnH5oLtzzXQqcFq5zdCbrdPdfu3ulu1cOGTKkjUOXTMxMqKBdpCOiDOAFwEQzG29m3QlCdl7zhcxsMlAKvJBy32gz6xVeLwXeD6xw943ALjN7b3j0w+VAdYSvQY7hlHEqaBfpiJKoVuzuh8zsy8DjQDHwW3dfamY/ABa6ezKMLwPu8qOLBaYAN5qZE0w7/NTdXwsf+wLwe6AX8OfwIjFQQbtIx0QWwADu/ijwaLP7rmt2+/o0z3sSmN7COhcC07I3SumIykQZDy2up7HJKS7SIdkibaEz4aRDZibKVNAu0k4KYOkQfVCnSPspgKVDkgXtC3Q8sEibKYClQ1TQLtJ+CmDpsJlhQXvtNhW0i7SFAlg6rDJZ0L5e88AibaEAlg5LFrRrHlikbRTA0mEqaBdpHwWwZEWyoH3bbhW0i2RKASxZoYJ2kbZTAEtWJAvaF+iNOJGMKYAlK1TQLtJ2CmDJmqpEmQraRdpAASxZU5Uo5WCjs3jD9riHItIlKIAla5IF7Qv1RpxIRhTAkjUDe3dn8rB+akYTyZACWLKqMlHKy+u20dikYh6R1iiAJauqEmXs2n+IFZtU0C7SGgWwZFWyoF3FPCKtUwBLVo0u7c1IFbSLZEQBLFmngnaRzCiAJeuqVNAukhEFsGSdCtpFMqMAlqybrIJ2kYwogCXrioqMynGlLHhDe8Aix6IAlkhUJspYtVkF7SLHogCWSKigXaR1CmCJhAraRVqnAJZI9OxWzPTRAzQPLHIMCmCJTGWijNfqdqigXaQFCmCJjAraRY5NASyRqRyXPCFDb8SJpKMAlsgM6N2NycP6MV/zwCJpKYAlUpWJUl5Zr4J2kXQUwBIpFbSLtEwBLJGqGh/MA+tz4kTeTQEskRo1sFdY0K4AFmlOASyRq0yUsWCdCtpFmlMAS+SqEqW8tXO/CtpFmlEAS+Q0DyySngJYIjdpqAraRdJRAEvkkgXtC7UHLHIUBbDkhAraRd5NASw5MXO8eiFEmlMAS06cNCooaNc0hMgRCmDJicMF7QrgLuPAoSbe2LI77mHkNQWw5IwK2ruWW59Zw5k3PqM2uwgpgCVnZo4PCtprVNDe6bk7Dyyqpcnh6rtr2LnvYNxDyksKYMmZU8aGb8RpGqLTW1y7g/Vv7+GT7xnLpp37+H710riHlJcUwJIzyYJ2nZDR+c1dVEf34iK+OesEvnLGBB5cVMe8xfVxDyvvKIAlp6rGq6C9szvU2MTDr27kjBOGMqBXN7784QmcPHYg33nwNeq2q88jmxTAklPJgvblm3bGPRRpwQtr32bLO/uZUzESgJLiIm65ZAZNTc4199Tol2cWKYAlpyoTyXlgTUN0VnMX1dOvRwkfPmHo4fvGDurN9bNP5MW1W/nNX9fGOLr8EmkAm9ksM1thZqvN7No0j99sZjXhZaWZbQ/vrzCzF8xsqZm9amaXpDznTDN7JXzO82Y2IcrXINmlgvbObd/BRh5fuolZ04bTs1vxUY994pTRnDttODc+sYIldTtiGmF+iSyAzawY+CVwLjAVuMzMpqYu4+5Xu3uFu1cAPwceCB/aA1zu7icCs4BbzGxg+NitwD+Gz/kT8N2oXoNEo2q8Cto7q6eXb+ad/YeYUzHqXY+ZGf/60ZMo69OdK+9axN4DOp67o6LcA54JrHb3te5+ALgLmHOM5S8D7gRw95Xuviq8Xg9sBoaEyznQP7w+ANBbs11MZaJMBe2d1NxFdQzp14P3HT8o7eOlfbpz4z9UsKZhN//252U5Hl3+iTKARwEbUm7Xhve9i5mNA8YDT6d5bCbQHVgT3vVZ4FEzqwU+BdyQxTFLDlQlSgEVtHc2O/Yc5JkVDVw4fSTFRdbich+YOJjPfmA8t72wnv9ZvjmHI8w/UQZwuu9gS39zXgrc5+5H/U1jZiOA24HPuHtTePfVwHnuPhr4HXBT2o2bfd7MFprZwoaGhna9AInGpKH96K+C9k7nz0s2cqCx6fDRD8fyfz4ymROG9+Pr9y1myzv7czC6/BRlANcCY1Juj6bl6YJLCacfksysP/AI8F13fzG8bwhQ7u4vhYvdDZyaboXu/mt3r3T3yiFDhqRbRGJSVGRUJsp0RlwnU11Tz/jBfZg+ekCry/bsVszPLp3Bzn2H+OZ9r2o+v52iDOAFwEQzG29m3QlCdl7zhcxsMlAKvJByX3fgQeA2d783ZfFtwAAzmxTePhvQRFQXVJkoVUF7J7Jpxz5efONtZpePxKzl6YdUk4f349pZJ/DU8s38af6bEY8wP0UWwO5+CPgy8DhBSN7j7kvN7AdmNjtl0cuAu/zoX6EXA6cDV6QcplYRrvNzwP1mtphgDvjrUb0GiU5VQgXtnclDi+txJ6Pph1RXnJrgtImD+eHDr7Om4Z2IRpe/rBD+dKisrPSFCxfGPQxJse9gI9Ovf4LPvD/Bt86bEvdwCt4FP/8rRWbM+/IH2vzct3buY9YtzzG6tDf3f+FUupfo/C7Svwf2LvpKSSx6diumfIwK2juD1ZvfYUndTmaXt23vN2lY/57828em81rdDm75y8osjy6/KYAlNipo7xzm1dRhRrsDGGDWtOFcUjmGW59dw0tr387i6PKbAlhiU5VQQXvc3J3qxfWcevwghvbv2aF1XXfhVMaV9eZr9yxWgXuGFMASm1PGlmGmgvY41WzYzvq396Q99bit+vQo4eZLKti0cx/XzV2ShdHlPwWwxEYF7fGrrqmne0kRs6YNz8r6Zowt5atnTGRuTT3VNXVZWWc+UwBLrCoTKmiPy+Hi9clD6d+zW9bW+6UPH88p40r57twl1G7bk7X15iMFsMRKBe3x+fuaoHj9ohntf/MtnZLiIm6+uAJ3uOaexfrlegwKYImVCtrjU11TT7+eJXxo8tDWF26jZIH7S29s5dfPqcC9JQpgidWogb0YNbAX8/VGXE4li9fPTVO8ni0fP3kU5500nJueVIF7SxTAErvKRCkLVdCeU08ta7l4PVuSBe6D+vTgqypwTyujADaz+83sfDNTYEvWqaA996pr6hjarwfvPS598Xq2DOzdnRsvLmdtw25+/OjrkW6rK8o0UG8FPgmsMrMbzOyECMckBUYF7bl1uHi9/NjF69ny/gmD+dxp47njxTd5evlbkW+vK8kogN39L+7+j8DJwDrgSTP7u5l9xsyyd/yKFKQjBe0K4FxoS/F6tiQL3L9x36sqcE+R8ZSCmQ0CriD4SKBFwM8IAvnJSEYmBSNZ0K4TMnJjbk0d4wf34aRRrRevZ0uPkmL+47KgwP0bKnA/LNM54AeAvwK9gQvdfba73+3uXwH6RjlAKQyViVJWb36HrSpoj9TGHXt56Y2tzKnIvHg9WyYN68e3zj2Bp5dv5o6XVOAOme8B/8Ldp7r7v7n7xtQH3L0ygnFJgUkWtL+sgvZIPbx4Y1i8Ht3RD8dyxakJTp80hB8/8jqrN6vAPdMAnmJmA5M3zKzUzL4Y0ZikAE0fPYDuJUWaB47Y3Jo6ykcPYPzgPrFs38z46Sem06tbMVfdvYgDh5paf1IeyzSAP+fuhzsD3X0bwUcDiWRFj5JiykeroD1KqzfvYmn9TmbHtPebNLR/T274+HSW1O3k5gIvcM80gIssZcLIzIqB7tEMSQpVZaKMJXU7dMB+RObV1FNkcOH0EXEPhY+cOJxLq8bwn8+u4cUCLnDPNIAfB+4xszPN7AyCj5B/LLphSSGamSjjYKOzuFYF7dnm7sytqefU4wd3uHg9W753QVjgfncNO/YWZoF7pgH8TeBp4AvAl4CngG9ENSgpTCePLcUMFryhaYhsq9mwnTe37mF2Do/9bU2fHiXccukM3tq1n+uqC7PAPdMTMZrc/VZ3/4S7f9zdf+Xu+jtRsupwQbuOhMi6bBevZ0vFmIFceeZEqgu0wD3T44Anmtl9Zva6ma1NXqIenBQeFbRnX1C8Xs+ZJ2S3eD1bvvihsMD9wcIrcM90CuJ3BH0Qh4APA7cBt0c1KClcVYky3lFBe1YFxesHcnrqcVuUFBdxyyUVOPC1uwurwD3TAO7l7k8B5u7r3f164IzohiWFKnlChuaBs2duTV1kxevZMqasN/939onMX7eVXz23Ju7h5EymAbwvrKJcZWZfNrOPAp33uyld1siwoF3zwNmx72Ajjy+Jtng9Wz528ijOnz6Cm55YyWu1hVHgnmkAX0XQA/FV4BTgn4BPRzUoKWwqaM+evyx7i90HGrko5pMvMmFm/PiiaQzu24Mr7y6MAvdWAzg86eJid3/H3Wvd/TPhkRAv5mB8UoCqVNCeNdU19Qzt14P3RFy8ni0De3fnpovLeWPLbn70SP4XuLcawOHhZqdYrquTpGAl54Hnax64Q4Li9c05K17PllMnDOZzpx3HH196k7+8nt8F7plOQSwCqs3sU2b2seQlyoFJ4Zo4tC/9e5awcL0CuCMeXbKRg43eJaYfmrvmnElMGdGfb97/Kg278rfAPdMALgPeJjjy4cLwckFUg5LCpoL27KiuqeO4wX2YNqp/3ENpsx4lxfzs0gre2X+Ib9y3OG/fDyjJZCF3/0zUAxFJVZUo4+nlm9m6+wBlfdT71FbJ4vWrzpyU8+L1bEkWuF//0Ovc8eJ6PvW+RNxDyrqMAtjMfge861eQu/+vrI9IhCMf1Llw3VbOObFznT7bFTy0uB53OlX3Q3t8+tQE/7OigR89soz3HT+ICUP7xT2krMp0CuJh4JHw8hTQH1CdvUTmpLCgfaGOB26X6pr6WIvXs8XM+Pd/mE6fHiVceVdN3hW4Z1rGc3/K5Y/AxcC0aIcmhUwF7e2XLF6P62OHsm1ov57c8LGTWFq/k5uezK8C94w/FbmZicDYbA5EpLkqFbS3S3VYvH5BJyhez5ZzThzOZTPH8Kvn1vDCmvwpcM+0DW2Xme1MXoCHCDqCRSJTFRa012xQQXum3J3qTla8ni3fu2AqiUF9uOaeGnbsyY8C90ynIPq5e/+UyyR3vz/qwUlhSxa0L9Q0RMYWhcXrnbX5rCN6dy/hlksq2LxrP9/LkwL3TPeAP2pmA1JuDzSzi6IblogK2ttjXli8/pFOVryeLeVjBnLVWROZt7ieuYu6foF7pnPA33f3w/VE4Sckfz+aIYkcUZUoU0F7hpLF62dN6ZzF69nyhQ9NoHJcKd+bu4QNW7t2gXvGn4qc5r6MjiEW6YjKRCnv7D/Eso0qaG/N38Li9dnl+XH0Q0uKi4ybwwL3a+7p2gXumQbwQjO7ycyON7PjzOxm4OUoByYCR4p5NA/cuurDxetD4h5K5MaU9eYHc4IC9/98tusWuGcawF8BDgB3A/cAewk+HVkkUipoz8zeA0Hx+nnTRnT64vVs+eiMUVwwfQQ3P7mSV2u75pEymR4Fsdvdr3X3yvDybXffHfXgRCA4LXnBGypoP5anlgfF6/l49ENLggL3kxjSrwdX3VXDngOH4h5Sm2V6FMSTZjYw5XapmT0e3bBEjqhMlLF51342bFVBe0vmLqpnWP+uU7yeLQN6d+PGi8t54+3d/OiRZXEPp80ynYIYHB75AIC7b0OfCSc5cviDOjUPnNb2PQd4duVmLpzetYrXs+XU4wfz+dOO408vvcmTXazAPdMAbjKzw6cem1mCNO1oIlGYOLQvA3p1U0F7C/68ZBMHGz1vuh/a42vnTGJqWOC+ede+uIeTsUwD+DvA82Z2u5ndDjwLfCu6YYkcUVRkVI4r1UcUtWDuojqOG9I1i9ezJVngvnv/Ib5536td5v2CTN+EewyoBFYQHAlxDcGRECI5UZkoY03Dbt5+J38/nqY96rfvZf66rcwpH9Vli9ezZeKwfnz7vCn8z4oGbn9xfdzDyUimb8J9lqAH+JrwcjtwfXTDEjlasqD9ZR2OdpSHXw2K1wvp6Idjufx94/jQ5CH8+JFlrHprV9zDaVWmUxBXAlXAenf/MDADaIhsVCLNqKA9vbmL6ikfM5BEFy9ezxYz4/994kiB+/5DnbvKNNMA3ufu+wDMrIe7LwcmRzcskaP1KCmmYvRAzQOnWPXWLl7fuJM55dr7TTW0X09+8vHpvL6x8xe4ZxrAteFxwHOBJ82sGqiPblgi71aZKFVBe4rDxevl+VO8ni1nTx3GZTPH8uvn1nbqAvdM34T7qLtvd/frge8B/w2ojlJyqipRxqEmFbRDWLy+uI73TxjM0H75VbyeLd+7YArjB/Xha524wL3NH0nk7s+6+zx3P9DasmY2y8xWmNlqM7s2zeM3m1lNeFlpZtvD+yvM7AUzW2pmr5rZJSnPMTP7cbj8MjP7altfg3RNJ49TQXvSog3b2bB1L7M1/dCi3t1LuOXSChp27ec7c1/rlIemRVYpaWbFwC+Bs4FaYIGZzXP315PLuPvVKct/heDNPYA9wOXuvsrMRgIvm9nj4dl4VwBjgBPcvcnMdEZegRjQKyhon68ApnpRHd1LipiVp8Xr2TJ99ECuPnsS//74Cs6cMpSPzhgd95CO0t4P5czETGC1u68N95bvAuYcY/nLgDsB3H2lu68Kr9cDm4Fkx94XgB+4e1P4+OaIxi+dULKg/VBjfn08eVsExesbOWvKUPrlcfF6tvzLB4+nKlHKdXOXdroC9ygDeBSwIeV2bXjfu5jZOGA88HSax2YC3YFk6efxwCVmttDM/mxmE7M6aunUKhOl7D7QyPJNnf8Yz6g8v3oLb+8+UNCnHrdFcZFx08UVAHztnppOVeAeZQCnOy2npVd+KXCfux/19raZjSA46eMzyT1eoAfBYXGVwG+A36bduNnnw5Be2NCgQ5bzxczxKmifV1NfMMXr2TKmrDc/uOhEFqzbxq3PrI57OIdFGcC1BHO1SaNp+dC1SwmnH5LMrD/wCPBdd3+x2XqTn8j8IDA93Qrd/dfJ/uIhQ/SDmi9GDAgL2tcV5gkZew808vjSoHi9R0lhFK9ny0UVo7iwfCS3/GUVizvJkTRRBvACYKKZjTez7gQhO6/5QmY2GSgFXki5rztBuN7m7vc2e8pc4Izw+geBzn2ktWRdVaKUBesKs6D9L8vC4vUZOvqhrcyMH82ZxtB+Pbjq7s5R4B5ZALv7IeDLwOPAMuAed19qZj8ws9kpi14G3OVH/2+6GDgduCLlMLWK8LEbgI+b2WvAvwFD7fUkAAAR/0lEQVSfjeo1SOdUyAXt1TVh8fr4wipez5agwL2CdW/v5ocPx1/gHuknG7v7o8Cjze67rtnt69M87w7gjhbWuR04P3ujlK4mOQ+8YN1Wxg7qHfNocidZvH7FqYmCLF7PlvcdP4jPn34cv3p2LR+ePIRzTozvUL4opyBEIjFhSFDQXmifkPHoaypez5Zrzp7MiSP7c+0Dr8Va4K4Ali4nWdBeaAFcXRMUr584snCL17Ole0nR4QL3r98bX4G7Ali6pEIraK/fvpeX3tjKRRUqXs+WCUP78Z3zp/DsygZueyGeAncFsHRJM8cXVkH7Q4uDIzjV/ZBdn3rvOD48eQj/+mg8Be4KYOmSpo0KCtoLZRqiuqaeChWvZ11Q4F5O3x4lfDWGAncFsHRJyYL2Qjgh43Dxuj52KBJD+vXgJx+fzrKNO7nxidyeVqAAli6rUArak8Xr509X8XpUzpo6jE++Zyy/+eta/r56S862qwCWLqtqfFDQvmhD/u4Fq3g9d757flDgfs29i3NW4K4Ali7r5LHJgvb8DeBX3gyK13Xsb/R6dy/hZ5fOoGHXfr6dowJ3BbB0WcmC9nx+I25eTR09Sor4yInD4h5KQThp9ACuPnsSj7y6kQdeqYt8ewpg6dLyuaD94OHi9WEqXs+hf/ng8cxMlPH9edEXuCuApUurGl+WtwXtfwuL12fr6IecKi4ybrqkHAOuvrsm0l/uCmDp0qoSwQkZ+TgNMa+mnv4qXo/F6NLe/PCiaSxcv41bn1nT+hPaSQEsXVqyoD3f3og7XLx+korX43LRjFHMLh/JXQs2sO9gNIc6RlpHKZILVYlS/r7mbdw9b3oSksXrmn6I148+Oo2mJqdnt2h+CWoPWLq8qvFBQfubnewTbzuiuqaO4f17qng9Zv17dmNg7+6RrV8BLF1eVSJZ0J4f0xDbdh/gmRUNXFg+QsXreU4BLF1esqA9Xz4p+c9LNnGoScXrhUABLF1evhW0z62p43gVrxcEBbDkharx+VHQXr99L/Pf2MocFa8XBAWw5IXk8cALu3hB+7yweF3Vk4VBASx5IVnQ3tXngZPF6+MGqXi9ECiAJS/kQ0H7yrd2sWzjTi7S3m/BUABL3qgaHxS07zlwKO6htEt1TV1YvK4ALhQKYMkblYmgoL1mw/a4h9Jm7k51TT3vnzCYIf16xD0cyREFsOSNrlzQ/sqb26ndtpeLdOxvQVEAS94Y0KsbJwzv3yWPB64Oi9fPUfF6QVEAS16pSpR2uYL2g41NPPLqRs6aquL1QqMAlrxSmeh6Be3Ph8Xrc8r15luhUQBLXumKBe3J4vUPqni94CiAJa+MGNCL0aVdp6A9Wbx+/nQVrxciBbDknapEGfPXbc3Jx4p31JPL3mLPgUZml+voh0KkAJa8U5kopaGLFLTPO1y8Xhb3UCQGCmDJO12loD1ZvD67YiRFKl4vSApgyTsThvRlYO/OX9D+6JKNHGpyZuvoh4KlAJa8kyxon9/JA7i6pp4JQ/uqeL2AKYAlL1UmyljbiQva65LF6+UjVbxewBTAkpc6e0H7Q2Hxuj52vrApgCUvTRs1gB6duKB97qI6ZoxV8XqhUwBLXupRUkz5mIHM74RHQqzYtIvlm3bp1GNRAEv+qkqUsrQTFrTPW1xHcZGpeF0UwJK/OmNBu4rXJZUCWPLWKeOCgvYFb3SeaYhX3txG7ba9mn4QQAEseax/z6CgfeH6zvNGXHVNPT1KivjItOFxD0U6AQWw5LXOVNB+sLGJh8Pi9b49SuIejnQCCmDJa52poP351VvYuvuAPvdNDlMAS15LnpAx/434pyGqF9UxoFc3PjhJxesSUABLXjtc0B7zPPCeA4d44vW3OO+k4XQv0X87CegnQfJeVaKMBeu2xVrQ/pdlm9lzoJE5mn6QFApgyXudoaC9elEdIwb0ZGZCxetyhAJY8l4y9OKaB962+wDPrmxgdrmK1+VoCmDJe8cfLmiP54SMR14Li9fVfCbNKIAl7yUL2hfE9EbcvLB4feoIFa/L0SINYDObZWYrzGy1mV2b5vGbzawmvKw0s+3h/RVm9oKZLTWzV83skjTP/bmZvRPl+CV/xFXQXrd9L/PXbeWiChWvy7tFdjqOmRUDvwTOBmqBBWY2z91fTy7j7lenLP8VYEZ4cw9wubuvMrORwMtm9ri7JwO6EhgY1dgl/6R+UOesHJ4GPK8mLF7Xx85LGlHuAc8EVrv7Wnc/ANwFzDnG8pcBdwK4+0p3XxVerwc2A0PgcLD/O/CNCMcueWbaqP6xFLRX1wTF62MH9c7pdqVriDKARwEbUm7Xhve9i5mNA8YDT6d5bCbQHVgT3vVlYJ67b8zqaCWvJQvaF+TwI4qSxes69VhaEmUAp5vwaulI+EuB+9y98agVmI0Abgc+4+5N4XTEPwA/b3XjZp83s4VmtrChoaGNQ5d8lOuC9uqaZPH6iJxsT7qeKAO4FhiTcns0UN/CspcSTj8kmVl/4BHgu+7+Ynj3DGACsNrM1gG9zWx1uhW6+6/dvdLdK4cM0bn3EswDH2pyat6MvqA9Wbz+gQmDGdxXxeuSXpQBvACYaGbjzaw7QcjOa76QmU0GSoEXUu7rDjwI3Obu9ybvd/dH3H24uyfcPQHscfcJEb4GySMnJwvac3A88Mvrt1G3fS9zdOyvHENkAezuhwjmax8HlgH3uPtSM/uBmc1OWfQy4C4/+kT9i4HTgStSDlOriGqsUhhyWdBeXVNPz25FnHOiitelZZG2Qrv7o8Cjze67rtnt69M87w7gjgzW37eDQ5QCMzNRyn0v13KosYmS4mj2Pw42NvHIaxs5a4qK1+XYdCacFJRkQfuyjdEVtD+/KiheV/OZtEYBLAWlMixoXxDh8cDVNSpel8wogKWgRF3QfqR4fYSK16VV+gmRgjMzwoL2J19/Kyxe19EP0joFsBScykQZDbv2s/7t7Be0z6upV/G6ZEwBLAWnKqJ54K0qXpc2UgBLwYmqoP3RsHhdRz9IphTAUnCCgvayrBe0z6upZ+LQvkwZ0S+r65X8pQCWglSVKGVtw262ZKmgvXbbHuav28ocFa9LGyiApSBVhm+SZWsa4qHFQTuqph+kLRTAUpBOGjUgqwXt1TV1nDx2IGPKVLwumVMAS0HqXlJExZiBWTkSYvmmnSzftEt7v9JmCmApWFWJMpbU7+xwQfu8mnoVr0u7KIClYFUmSmnsYEF7U5OK16X9FMBSsLJR0P7Km0Hx+kUzdOqxtJ0CWApW/57dmDK8f4fmgefW1NGzWxFnT1XxurSdAlgKWlWilFfe3MahxqY2P/dgYxOPvKridWk/BbAUtMpEGXvaWdD+/KotbNtzUB87L+2mAJaCVhWekNGeaYi5NXUM7N2N01W8Lu2kAJaCNnxAT8aU9WpzAO85cIgnlqp4XTpGPzlS8KrGtb2g/cnX32LvwUbmlOvoB2k/BbAUvMpEGVveaVtBe3VNPSMH9Dw8hSHSHgpgKXhtLWjfuvsAz61s4MIKFa9LxyiApeBNGNqX0t7dMg7gR5LF6+U6+kE6RgEsBc/MOGVcWcbVlPNq6pg0TMXr0nEKYBHCgvYtrRe0127bw4J125hTMUrF69JhCmARMi9on7e4HoDZOvpBskABLMKRgvbW5oHn1dRzyrhSFa9LViiARThS0H6sT8g4UryuvV/JDgWwSKi1gvbqsHj9vJNUvC7ZoQAWCVWNL2uxoL2pyZlXU89pE1W8LtmjABYJnTx2IEUG89NMQ7wcFq9r+kGySQEsEurXsxsnDO+f9kiI6rB4/RwVr0sWKYBFUqQraE8Wr589dTh9VLwuWaQAFklRNf7dBe1/XdXAtj0H1XwmWacAFklROS44ISN1Hri6pl7F6xIJBbBIimRBe/J44N37Vbwu0dFPlEgzqQXtf1kWFK/rc98kCgpgkWaqxh8paE8Wr1eOK417WJKHFMAizSQL2h9fuknF6xIpBbBIM8cPCQraf/H0ag41uaYfJDIKYJFmkgXtu/YfYtKwvpwwXMXrEg0FsEgaM8cH0xAqXpcoKYBF0vjIicOpSpTyiVNGxz0UyWM6r1IkjXGD+nDvv5wa9zAkz2kPWEQkJgpgEZGYKIBFRGKiABYRiYkCWEQkJgpgEZGYKIBFRGKiABYRiYkCWEQkJgpgEZGYKIBFRGKiABYRiYm5e9xjiJyZNQDr2/i0wcCWCIbTVbbfGcZQ6NvvDGOIe/udYQzt2f4Wd5/V2kIFEcDtYWYL3b2yULffGcZQ6NvvDGOIe/udYQxRbl9TECIiMVEAi4jERAHcsl8X+PYh/jEU+vYh/jHEvX2IfwyRbV9zwCIiMdEesIhITBTAKcxsjJn9j5ktM7OlZnZlDGPoaWbzzWxxOIb/m+sxhOMoNrNFZvZwTNtfZ2avmVmNmS2MYfsDzew+M1se/jy8L8fbnxy+9uRlp5ldleMxXB3+DC4xszvNrGeOt39luO2luXrtZvZbM9tsZktS7iszsyfNbFX4b2m2tqcAPtoh4Bp3nwK8F/iSmU3N8Rj2A2e4ezlQAcwys/fmeAwAVwLLYthuqg+7e0VMhyD9DHjM3U8Aysnx18LdV4SvvQI4BdgDPJir7ZvZKOCrQKW7TwOKgUtzuP1pwOeAmQRf/wvMbGIONv17oPnxu9cCT7n7ROCp8HZWKIBTuPtGd38lvL6L4D/dqByPwd39nfBmt/CS04l6MxsNnA/8Vy6321mYWX/gdOC/Adz9gLtvj3FIZwJr3L2tJxN1VAnQy8xKgN5AfQ63PQV40d33uPsh4Fngo1Fv1N2fA7Y2u3sO8Ifw+h+Ai7K1PQVwC8wsAcwAXoph28VmVgNsBp5091yP4RbgG0BTjrebyoEnzOxlM/t8jrd9HNAA/C6chvkvM+uT4zGkuhS4M5cbdPc64KfAm8BGYIe7P5HDISwBTjezQWbWGzgPGJPD7aca5u4bIdhJA4Zma8UK4DTMrC9wP3CVu+/M9fbdvTH803M0MDP8cywnzOwCYLO7v5yrbbbg/e5+MnAuwVTQ6TncdglwMnCru88AdpPFPzvbwsy6A7OBe3O83VKCPb/xwEigj5n9U6627+7LgJ8ATwKPAYsJpgjzigK4GTPrRhC+f3T3B+IcS/hn7zO8e04qSu8HZpvZOuAu4AwzuyOH2wfA3evDfzcTzH3OzOHma4HalL887iMI5DicC7zi7m/leLtnAW+4e4O7HwQeAE7N5QDc/b/d/WR3P51gWmBVLref4i0zGwEQ/rs5WytWAKcwMyOY91vm7jfFNIYhZjYwvN6L4D/C8lxt392/5e6j3T1B8Kfv0+6esz0fADPrY2b9kteBcwj+JM0Jd98EbDCzyeFdZwKv52r7zVxGjqcfQm8C7zWz3uH/izPJ8RuRZjY0/Hcs8DHi+ToAzAM+HV7/NFCdrRWXZGtFeeL9wKeA18I5WIBvu/ujORzDCOAPZlZM8AvyHneP5VCwGA0DHgz+31MC/MndH8vxGL4C/DGcAlgLfCbH2yec+zwb+N+53ra7v2Rm9wGvEPzpv4jcn5F2v5kNAg4CX3L3bVFv0MzuBD4EDDazWuD7wA3APWb2zwS/mP4ha9vTmXAiIvHQFISISEwUwCIiMVEAi4jERAEsIhITBbCISEwUwCIiMVEAi7SDmT0aVlYONLMvxj0e6Zp0HLBIB4SlTQ+HlY0ibaI9YMlLZpYIi9R/ExZ6PxGe2p1u2WfMrDK8PjjswcDMrjCzB8zssbCM+/+lPGedmQ0mOEvq+LA0/d/NbISZPRfeXmJmp+Xg5UoXpQCWfDYR+KW7nwhsBz7ejnVUAJcAJwGXmFnzSsRrCbp6K9z968AngcfDNrtyoAaRFqgLQvLZG+6eDMCXgUQ71vGUu+8AMLPXgXHAhmMsvwD4bdiqNzdl+yLvoj1gyWf7U6430vIOxyGO/F9o/rlnma4DOPyJCqcDdcDtZnZ5xqOVgqMAFoF1BJ+7BvCJNj53F9AvecPMxhEU2v+GoNo0rh5h6QI0BSESfPTOPWb2KeDptjzR3d82s7+Fn6L7Z4Le4q+b2UHgHUB7wNIiHYYmIhITTUGIiMREUxBSMMzslwSfepLqZ+7+uzjGI6IpCBGRmGgKQkQkJgpgEZGYKIBFRGKiABYRiYkCWEQkJv8fQb3WucPz6VQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.relplot(x=\"n_units\", y=\"accuracy\", data=tmp_df, kind=\"line\")\n",
    "plt.title(\"Accuracy on validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.7570148\n",
      "Test accuracy:  0.74459636\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.loc[:, feature_subset]\n",
    "test_df = test_df.loc[:, feature_subset]\n",
    "input_nodes = 13 # maybe 75 for a bias\n",
    "\n",
    "train_features_array = train_df.iloc[:, :-1].values\n",
    "train_labels_array = train_df.loc[:, \"target Feature\"].values\n",
    "train_labels_binarized = label_binarize(train_labels_array, classes=[0,1,2])\n",
    "test_features_array = test_df.iloc[:, :-1].values\n",
    "test_labels_array = test_df.loc[:, \"target Feature\"].values\n",
    "test_labels_binarized = label_binarize(test_labels_array, classes=[0,1,2])\n",
    "\n",
    "normalizer = MinMaxScaler((0,1))\n",
    "normalizer.fit(train_features_array)\n",
    "\n",
    "train_features_normalized = normalizer.transform(train_features_array)\n",
    "\n",
    "weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([input_nodes, optimal_n_units]), name=\"h1\"),\n",
    "        'output': tf.Variable(tf.random_normal([optimal_n_units, output_nodes]), name=\"output_weights\")\n",
    "    }\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([optimal_n_units]), name=\"b1\"),\n",
    "    'output': tf.Variable(tf.random_normal([output_nodes]), name=\"output_bias\")\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Finding the cost of the algorithm\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "# Finding optimization of the algorithm\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(1, 1001):\n",
    "        avg_cost = 0.0\n",
    "        _, c = sess.run([optimizer, cost], \n",
    "                        feed_dict={\n",
    "                            X: train_features_normalized, \n",
    "                            Y: train_labels_binarized, \n",
    "                        })\n",
    "        avg_cost += c / total_batch\n",
    "#             if i % 100 == 0:\n",
    "#                 print(\"Epoch:\", '%04d' % (i), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "#         print(\"Optimization Finished!\")            \n",
    "\n",
    "#         print()\n",
    "\n",
    "    train_pred = tf.nn.softmax(logits)\n",
    "    correct_train_prediction = tf.equal(tf.argmax(train_pred, 1), tf.argmax(Y, 1))\n",
    "    train_accuracy = tf.reduce_mean(tf.cast(correct_train_prediction, \"float\"))\n",
    "    print(\"Training accuracy: \", train_accuracy.eval({X: train_features_normalized, Y: train_labels_binarized}))\n",
    "\n",
    "    test_pred = tf.nn.softmax(logits)\n",
    "    correct_test_prediction = tf.equal(tf.argmax(test_pred, 1), tf.argmax(Y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_test_prediction, \"float\"))\n",
    "    test_accuracy_value = test_accuracy.eval({X: test_features_normalized, Y: test_labels_binarized})\n",
    "    print(\"Test accuracy: \", test_accuracy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning network with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 2 units in layer 1 and 2 units in layer 2\n",
      "Training accuracy:  0.7588965\n",
      "Validation accuracy:  0.72540385\n",
      "------------------------------\n",
      "Trying 2 units in layer 1 and 3 units in layer 2\n",
      "Training accuracy:  0.75954944\n",
      "Validation accuracy:  0.7283407\n",
      "------------------------------\n",
      "Trying 2 units in layer 1 and 4 units in layer 2\n",
      "Training accuracy:  0.75987595\n",
      "Validation accuracy:  0.72540385\n",
      "------------------------------\n",
      "Trying 2 units in layer 1 and 5 units in layer 2\n",
      "Training accuracy:  0.7590597\n",
      "Validation accuracy:  0.7312775\n",
      "------------------------------\n",
      "Trying 2 units in layer 1 and 6 units in layer 2\n",
      "Training accuracy:  0.7597127\n",
      "Validation accuracy:  0.7283407\n",
      "------------------------------\n",
      "Trying 4 units in layer 1 and 2 units in layer 2\n",
      "Training accuracy:  0.76216125\n",
      "Validation accuracy:  0.7312775\n",
      "------------------------------\n",
      "Trying 4 units in layer 1 and 3 units in layer 2\n",
      "Training accuracy:  0.759223\n",
      "Validation accuracy:  0.7298091\n",
      "------------------------------\n",
      "Trying 4 units in layer 1 and 4 units in layer 2\n",
      "Training accuracy:  0.590271\n",
      "Validation accuracy:  0.56828195\n",
      "------------------------------\n",
      "Trying 4 units in layer 1 and 5 units in layer 2\n",
      "Training accuracy:  0.7602024\n",
      "Validation accuracy:  0.72540385\n",
      "------------------------------\n",
      "Trying 4 units in layer 1 and 6 units in layer 2\n",
      "Training accuracy:  0.75514203\n",
      "Validation accuracy:  0.72687227\n",
      "------------------------------\n",
      "Trying 6 units in layer 1 and 2 units in layer 2\n",
      "Training accuracy:  0.759223\n",
      "Validation accuracy:  0.72246695\n",
      "------------------------------\n",
      "Trying 6 units in layer 1 and 3 units in layer 2\n",
      "Training accuracy:  0.7571009\n",
      "Validation accuracy:  0.7209985\n",
      "------------------------------\n",
      "Trying 6 units in layer 1 and 4 units in layer 2\n",
      "Training accuracy:  0.7605289\n",
      "Validation accuracy:  0.72687227\n",
      "------------------------------\n",
      "Trying 6 units in layer 1 and 5 units in layer 2\n",
      "Training accuracy:  0.7575906\n",
      "Validation accuracy:  0.7209985\n",
      "------------------------------\n",
      "Trying 6 units in layer 1 and 6 units in layer 2\n",
      "Training accuracy:  0.7590597\n",
      "Validation accuracy:  0.7298091\n",
      "------------------------------\n",
      "Optimal number of units in first hidden layer: 2\n",
      "Optimal number of units in second hidden layer: 5\n",
      "Maximum validation accuracy: 0.7312775254249573\n"
     ]
    }
   ],
   "source": [
    "total_batch = train_features_array.shape[0]\n",
    "max_validation_accuracy = 0\n",
    "optimal_weights = {}\n",
    "optimal_n_units_1 = 0\n",
    "optimal_n_units_2 = 0\n",
    "\n",
    "n_layer_one_units = [i for i in range(2, 7, 2)]\n",
    "n_layer_two_units = [i for i in range(2, 7)]\n",
    "accuracies = {}\n",
    "\n",
    "for n_unit_1 in n_layer_one_units:\n",
    "    for n_unit_2 in n_layer_two_units:\n",
    "        print(\"Trying {} units in layer 1 and {} units in layer 2\".format(n_unit_1, n_unit_2))\n",
    "\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([input_nodes, n_unit_1]), name=\"h1\"),\n",
    "            'h2': tf.Variable(tf.random_normal([n_unit_1, n_unit_2])),\n",
    "            'output': tf.Variable(tf.random_normal([n_unit_2, output_nodes]), name=\"output_weights\")\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([n_unit_1]), name=\"b1\"),\n",
    "            'b2': tf.Variable(tf.random_normal([n_unit_2])),\n",
    "            'output': tf.Variable(tf.random_normal([output_nodes]), name=\"output_bias\")\n",
    "        }\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net_2_layers(X)\n",
    "\n",
    "        # Finding the cost of the algorithm\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "        # Finding optimization of the algorithm\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for i in range(1, 1001):\n",
    "                avg_cost = 0.0\n",
    "                _, c = sess.run([optimizer, cost], \n",
    "                                feed_dict={\n",
    "                                    X: train_features_normalized, \n",
    "                                    Y: train_labels_binarized, \n",
    "                                })\n",
    "                avg_cost += c / total_batch\n",
    "    #             if i % 100 == 0:\n",
    "    #                 print(\"Epoch:\", '%04d' % (i), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    #         print(\"Optimization Finished!\")            \n",
    "\n",
    "    #         print()\n",
    "\n",
    "            train_pred = tf.nn.softmax(logits)\n",
    "            correct_train_prediction = tf.equal(tf.argmax(train_pred, 1), tf.argmax(Y, 1))\n",
    "            train_accuracy = tf.reduce_mean(tf.cast(correct_train_prediction, \"float\"))\n",
    "            print(\"Training accuracy: \", train_accuracy.eval({X: train_features_normalized, Y: train_labels_binarized}))\n",
    "\n",
    "            valid_pred = tf.nn.softmax(logits)\n",
    "            correct_valid_prediction = tf.equal(tf.argmax(valid_pred, 1), tf.argmax(Y, 1))\n",
    "            valid_accuracy = tf.reduce_mean(tf.cast(correct_valid_prediction, \"float\"))\n",
    "            valid_accuracy_value = valid_accuracy.eval({X: vald_features_normalized, Y: vald_labels_binarized})\n",
    "            accuracies[(n_unit_1, n_unit_2)] = valid_accuracy_value\n",
    "            print(\"Validation accuracy: \", valid_accuracy_value)\n",
    "\n",
    "            if max_validation_accuracy < valid_accuracy_value:\n",
    "                max_validation_accuracy = valid_accuracy_value\n",
    "                optimal_n_units_1 = n_unit_1\n",
    "                optimal_n_units_2 = n_unit_2\n",
    "                optimal_weights[\"h1\"] = weights[\"h1\"].eval()\n",
    "                optimal_weights[\"h2\"] = weights[\"h2\"].eval()\n",
    "                optimal_weights[\"output_weights\"] = weights[\"output\"].eval()\n",
    "                optimal_weights[\"b1\"] = biases[\"b1\"].eval()\n",
    "                optimal_weights[\"b2\"] = biases[\"b1\"].eval()\n",
    "                optimal_weights[\"output_bias\"] = biases[\"output\"].eval()\n",
    "\n",
    "        print(\"------------------------------\")\n",
    "\n",
    "print(\"Optimal number of units in first hidden layer: {}\".format(optimal_n_units_1))\n",
    "print(\"Optimal number of units in second hidden layer: {}\".format(optimal_n_units_2))\n",
    "print(\"Maximum validation accuracy: {}\".format(max_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.7561334\n",
      "Test accuracy:  0.74459636\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.loc[:, feature_subset]\n",
    "test_df = test_df.loc[:, feature_subset]\n",
    "input_nodes = 13 # maybe 75 for a bias\n",
    "\n",
    "train_features_array = train_df.iloc[:, :-1].values\n",
    "train_labels_array = train_df.loc[:, \"target Feature\"].values\n",
    "train_labels_binarized = label_binarize(train_labels_array, classes=[0,1,2])\n",
    "\n",
    "normalizer = MinMaxScaler((0,1))\n",
    "normalizer.fit(train_features_array)\n",
    "\n",
    "train_features_normalized = normalizer.transform(train_features_array)\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([input_nodes, optimal_n_units_1]), name=\"h1\"),\n",
    "    'h2': tf.Variable(tf.random_normal([optimal_n_units_1, optimal_n_units_2])),\n",
    "    'output': tf.Variable(tf.random_normal([optimal_n_units_2, output_nodes]), name=\"output_weights\")\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([optimal_n_units_1]), name=\"b1\"),\n",
    "    'b2': tf.Variable(tf.random_normal([optimal_n_units_2])),\n",
    "    'output': tf.Variable(tf.random_normal([output_nodes]), name=\"output_bias\")\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net_2_layers(X)\n",
    "\n",
    "# Finding the cost of the algorithm\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "# Finding optimization of the algorithm\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(1, 1001):\n",
    "        avg_cost = 0.0\n",
    "        _, c = sess.run([optimizer, cost], \n",
    "                        feed_dict={\n",
    "                            X: train_features_normalized, \n",
    "                            Y: train_labels_binarized, \n",
    "                        })\n",
    "        avg_cost += c / total_batch\n",
    "#             if i % 100 == 0:\n",
    "#                 print(\"Epoch:\", '%04d' % (i), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "#         print(\"Optimization Finished!\")            \n",
    "\n",
    "#         print()\n",
    "\n",
    "    train_pred = tf.nn.softmax(logits)\n",
    "    correct_train_prediction = tf.equal(tf.argmax(train_pred, 1), tf.argmax(Y, 1))\n",
    "    train_accuracy = tf.reduce_mean(tf.cast(correct_train_prediction, \"float\"))\n",
    "    print(\"Training accuracy: \", train_accuracy.eval({X: train_features_normalized, Y: train_labels_binarized}))\n",
    "          \n",
    "    test_pred = tf.nn.softmax(logits)\n",
    "    correct_test_prediction = tf.equal(tf.argmax(test_pred, 1), tf.argmax(Y, 1))\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(correct_test_prediction, \"float\"))\n",
    "    print(\"Test accuracy: \", test_accuracy.eval({X: test_features_normalized, Y: test_labels_binarized}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
