{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_1, weights['output']) + biases['output']\n",
    "    return out_layer\n",
    "\n",
    "def neural_net_2_layers(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['output']) + biases['output']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file):\n",
    "    filename = [file]\n",
    "    record_defaults = [tf.float32] * 75   # Eight required float columns\n",
    "    dataset = tf.contrib.data.CsvDataset(filename, record_defaults)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Data/formated.csv\")\n",
    "test_df = pd.read_csv(\"Data/formatedTesting.csv\")\n",
    "\n",
    "train_df.columns = [col.strip() for col in train_df.columns]\n",
    "test_df.columns = [col.strip() for col in test_df.columns]\n",
    "\n",
    "input_nodes = 72 # maybe 75 for a bias\n",
    "output_nodes = 3\n",
    "learning_rate = 0.01\n",
    "X = tf.placeholder(\"float\", [None, input_nodes])\n",
    "Y = tf.placeholder(\"float\", [None, output_nodes])\n",
    "\n",
    "# Delete duplicate columns\n",
    "del train_df[\"e1\"]\n",
    "del train_df[\"e1.1\"]\n",
    "del test_df[\"e1\"]\n",
    "del test_df[\"e1.1\"]\n",
    "\n",
    "# Uncomment this block to try out different set of features\n",
    "# feature_subset = [\"assist count\", \"assist count 2\", \"shots made\", \"e6\", \"is goal 2\", \"s1.1\",\n",
    "#                             \"e6.1\", \"is goal\", \"e4\", \"e2\", \"shots made 2\", \"s2.1\", \"e2.1\", \"target Feature\"]\n",
    "# train_df = train_df.loc[:, feature_subset]\n",
    "# test_df = test_df.loc[:, feature_subset]\n",
    "# input_nodes = 13 # maybe 75 for a bias\n",
    "\n",
    "# Split the dataframe into features and labels arrays\n",
    "train_features_array = train_df.iloc[:, :-1].values\n",
    "train_labels_array = train_df.loc[:, \"target Feature\"].values\n",
    "train_labels_binarized = label_binarize(train_labels_array, classes=[0,1,2])\n",
    "print(train_labels_binarized)\n",
    "test_features_array = test_df.iloc[:, :-1].values\n",
    "test_labels_array = test_df.loc[:, \"target Feature\"].values\n",
    "test_labels_binarized = label_binarize(test_labels_array, classes=[0,1,2])\n",
    "\n",
    "normalizer = MinMaxScaler((0,1))\n",
    "normalizer.fit(train_features_array)\n",
    "\n",
    "train_features_normalized = normalizer.transform(train_features_array)\n",
    "test_features_normalized = normalizer.transform(test_features_array)\n",
    "\n",
    "train_features_normalized, vald_features_normalized, train_labels_binarized, vald_labels_binarized = train_test_split(train_features_normalized, train_labels_binarized, test_size=0.10, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hLayers = 2\n",
    "# hLayerOne = 36 # half of input nodes\n",
    "# hLayerTwo = 36 # for sigmoid function\n",
    "\n",
    "\n",
    "# #epochs = 1 # number of times to iterate over data\n",
    "# batches = 3\n",
    "# train_batch = int(len(train_df)/batches)\n",
    "# test_batch = int(len(test_df)/batches)\n",
    "# # keep_prob = tf.placeholder(\"float\")\n",
    "# # print(keep_prob)\n",
    "\n",
    "# print(X)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning network with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 2 units in layer 1\n",
      "Training accuracy:  0.7588965\n",
      "Validation accuracy:  0.7195301\n",
      "------------------------------\n",
      "Trying 4 units in layer 1\n",
      "Training accuracy:  0.75775385\n",
      "Validation accuracy:  0.7180617\n",
      "------------------------------\n",
      "Trying 6 units in layer 1\n",
      "Training accuracy:  0.7639569\n",
      "Validation accuracy:  0.7048458\n",
      "------------------------------\n",
      "Trying 8 units in layer 1\n",
      "Training accuracy:  0.7588965\n",
      "Validation accuracy:  0.7195301\n",
      "------------------------------\n",
      "Trying 10 units in layer 1\n",
      "Training accuracy:  0.77440417\n",
      "Validation accuracy:  0.7077827\n",
      "------------------------------\n",
      "Optimal number of units in first hidden layer: 2\n",
      "Maximum validation accuracy: 0.7195301055908203\n"
     ]
    }
   ],
   "source": [
    "total_batch = train_features_array.shape[0]\n",
    "max_validation_accuracy = 0\n",
    "optimal_weights = {}\n",
    "optimal_n_units = 0\n",
    "\n",
    "n_layer_one_units = [i for i in range(2, 11, 2)]\n",
    "\n",
    "for n_unit_1 in n_layer_one_units:\n",
    "    print(\"Trying {} units in layer 1\".format(n_unit_1))\n",
    "\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([input_nodes, n_unit_1]), name=\"h1\"),\n",
    "        'output': tf.Variable(tf.random_normal([n_unit_1, output_nodes]), name=\"output_weights\")\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_unit_1]), name=\"b1\"),\n",
    "        'output': tf.Variable(tf.random_normal([output_nodes]), name=\"output_bias\")\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    logits = neural_net(X)\n",
    "\n",
    "    # Finding the cost of the algorithm\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "    # Finding optimization of the algorithm\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(1, 1001):\n",
    "            avg_cost = 0.0\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                X: train_features_normalized, \n",
    "                                Y: train_labels_binarized, \n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "#             if i % 100 == 0:\n",
    "#                 print(\"Epoch:\", '%04d' % (i), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "#         print(\"Optimization Finished!\")            \n",
    "\n",
    "#         print()\n",
    "\n",
    "        train_pred = tf.nn.softmax(logits)\n",
    "        correct_train_prediction = tf.equal(tf.argmax(train_pred, 1), tf.argmax(Y, 1))\n",
    "        train_accuracy = tf.reduce_mean(tf.cast(correct_train_prediction, \"float\"))\n",
    "        print(\"Training accuracy: \", train_accuracy.eval({X: train_features_normalized, Y: train_labels_binarized}))\n",
    "\n",
    "        valid_pred = tf.nn.softmax(logits)\n",
    "        correct_valid_prediction = tf.equal(tf.argmax(valid_pred, 1), tf.argmax(Y, 1))\n",
    "        valid_accuracy = tf.reduce_mean(tf.cast(correct_valid_prediction, \"float\"))\n",
    "        valid_accuracy_value = valid_accuracy.eval({X: vald_features_normalized, Y: vald_labels_binarized})\n",
    "        print(\"Validation accuracy: \", valid_accuracy_value)\n",
    "\n",
    "        if max_validation_accuracy < valid_accuracy_value:\n",
    "            max_validation_accuracy = valid_accuracy_value\n",
    "            optimal_n_units = n_unit_1\n",
    "            optimal_weights[\"h1\"] = weights[\"h1\"].eval()\n",
    "            optimal_weights[\"output_weights\"] = weights[\"output\"].eval()\n",
    "            optimal_weights[\"b1\"] = biases[\"b1\"].eval()\n",
    "            optimal_weights[\"output_bias\"] = biases[\"output\"].eval()\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "print(\"Optimal number of units in first hidden layer: {}\".format(optimal_n_units))\n",
    "print(\"Maximum validation accuracy: {}\".format(max_validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning network with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch = train_features_array.shape[0]\n",
    "max_validation_accuracy = 0\n",
    "optimal_weights = {}\n",
    "optimal_n_units_1 = 0\n",
    "optimal_n_units_2 = 0\n",
    "\n",
    "n_layer_one_units = [i for i in range(2, 11, 2)]\n",
    "n_layer_two_units = [i for i in range(2, 7)]\n",
    "\n",
    "for n_unit_1 in n_layer_one_units:\n",
    "    for n_unit_2 in n_layer_two_units:\n",
    "        print(\"Trying {} units in layer 1 and {} units in layer 2\".format(n_unit_1, n_unit_2))\n",
    "\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([input_nodes, n_unit_1]), name=\"h1\"),\n",
    "            'h2': tf.Variable(tf.random_normal([n_unit_1, n_unit_2])),\n",
    "            'output': tf.Variable(tf.random_normal([n_unit_2, output_nodes]), name=\"output_weights\")\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([n_unit_1]), name=\"b1\"),\n",
    "            'b2': tf.Variable(tf.random_normal([n_unit_2])),\n",
    "            'output': tf.Variable(tf.random_normal([output_nodes]), name=\"output_bias\")\n",
    "        }\n",
    "\n",
    "        # Construct model\n",
    "        logits = neural_net_2_layers(X)\n",
    "\n",
    "        # Finding the cost of the algorithm\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "        # Finding optimization of the algorithm\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for i in range(1, 1001):\n",
    "                avg_cost = 0.0\n",
    "                _, c = sess.run([optimizer, cost], \n",
    "                                feed_dict={\n",
    "                                    X: train_features_normalized, \n",
    "                                    Y: train_labels_binarized, \n",
    "                                })\n",
    "                avg_cost += c / total_batch\n",
    "    #             if i % 100 == 0:\n",
    "    #                 print(\"Epoch:\", '%04d' % (i), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    #         print(\"Optimization Finished!\")            \n",
    "\n",
    "    #         print()\n",
    "\n",
    "            train_pred = tf.nn.softmax(logits)\n",
    "            correct_train_prediction = tf.equal(tf.argmax(train_pred, 1), tf.argmax(Y, 1))\n",
    "            train_accuracy = tf.reduce_mean(tf.cast(correct_train_prediction, \"float\"))\n",
    "            print(\"Training accuracy: \", train_accuracy.eval({X: train_features_normalized, Y: train_labels_binarized}))\n",
    "\n",
    "            valid_pred = tf.nn.softmax(logits)\n",
    "            correct_valid_prediction = tf.equal(tf.argmax(valid_pred, 1), tf.argmax(Y, 1))\n",
    "            valid_accuracy = tf.reduce_mean(tf.cast(correct_valid_prediction, \"float\"))\n",
    "            valid_accuracy_value = valid_accuracy.eval({X: vald_features_normalized, Y: vald_labels_binarized})\n",
    "            print(\"Validation accuracy: \", valid_accuracy_value)\n",
    "\n",
    "            if max_validation_accuracy < valid_accuracy_value:\n",
    "                max_validation_accuracy = valid_accuracy_value\n",
    "                optimal_n_units_1 = n_unit_1\n",
    "                optimal_n_units_2 = n_unit_2\n",
    "                optimal_weights[\"h1\"] = weights[\"h1\"].eval()\n",
    "                optimal_weights[\"h2\"] = weights[\"h2\"].eval()\n",
    "                optimal_weights[\"output_weights\"] = weights[\"output\"].eval()\n",
    "                optimal_weights[\"b1\"] = biases[\"b1\"].eval()\n",
    "                optimal_weights[\"b2\"] = biases[\"b1\"].eval()\n",
    "                optimal_weights[\"output_bias\"] = biases[\"output\"].eval()\n",
    "\n",
    "        print(\"------------------------------\")\n",
    "\n",
    "print(\"Optimal number of units in first hidden layer: {}\".format(optimal_n_units_1))\n",
    "print(\"Optimal number of units in second hidden layer: {}\".format(optimal_n_units_2))\n",
    "print(\"Maximum validation accuracy: {}\".format(max_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
